{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import xml.etree.ElementTree as xml\n",
    "import xml.etree.cElementTree as ET\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "def load_data(filename):\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No file found!\")\n",
    "        data = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(textbody):\n",
    "    text = BeautifulSoup(textbody).text\n",
    "    text = re.sub(r'[-+]?\\d+', '', text)\n",
    "    text=text.translate((str.maketrans('','',string.punctuation)))  \n",
    "    return wordprocess(text)\n",
    "    #return wordprocess(TAG_RE.sub('', text))\n",
    "\n",
    "def wordprocess(text):    \n",
    "    # https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "    # https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "    # https://www.geeksforgeeks.org/python-lemmatization-with-nltk    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    new_words= [porter.stem(lemmatizer.lemmatize(word)) for word in words if not word in stop_words]\n",
    "    return new_words\n",
    "\n",
    "\n",
    "# Parse XML with ElementTree\n",
    "def parseXML(file_name):\n",
    "    tree = ET.ElementTree(file=file_name)\n",
    "    root = tree.getroot()\n",
    "    rows = root.getchildren()\n",
    "    iter = 0\n",
    "    large_vocab = []\n",
    "    for row in rows:\n",
    "        textbody = row.attrib['Body']\n",
    "        file_vocab = remove_tags(textbody.lower())        \n",
    "        #print(textbody,cleantext,file_vocab)\n",
    "        large_vocab.extend(file_vocab)        \n",
    "        iter += 1\n",
    "        if iter == 500:\n",
    "            large_vocab = list(set(large_vocab))\n",
    "            print(len(large_vocab))\n",
    "            return large_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_creation():\n",
    "    all_files= glob.glob('./Data/Data/Training/*.xml')\n",
    "    final_vocab = []\n",
    "    for each_file in all_files:\n",
    "        final_vocab.extend(parseXML(each_file))\n",
    "        final_vocab = list(set(final_vocab))\n",
    "    print(len(final_vocab))\n",
    "    save_data(final_vocab, \"vocab.pt\")\n",
    "    \n",
    "# vocab_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# Parse XML with ElementTree\n",
    "def idf_distance_measure(file_name, start_no, end_no, final_vocab):\n",
    "    tree = ET.ElementTree(file=file_name)\n",
    "    root = tree.getroot()\n",
    "    rows = root.getchildren()   \n",
    "    \n",
    "    # zero vector for counter\n",
    "    df_vector = np.zeros(len(final_vocab)) \n",
    "    \n",
    "    for iter in range(start_no, end_no):        \n",
    "        row = rows[iter]\n",
    "        textbody = row.attrib['Body']\n",
    "        doc_vocab = remove_tags(textbody.lower())      \n",
    "        doc_vocab = list(set(doc_vocab))  \n",
    "        \n",
    "        if len(doc_vocab) == 0:\n",
    "            continue\n",
    "               \n",
    "        # check if a word from vocab appears in the document\n",
    "        for vocab_iter, each_tok in enumerate(final_vocab):\n",
    "            if each_tok in doc_vocab:\n",
    "                df_vector[vocab_iter] += 1\n",
    "                \n",
    "    return np.copy(df_vector)   \n",
    "\n",
    "def idf_measure():\n",
    "    all_files=  glob.glob('./Data/Data/Training/*.xml')\n",
    "    total_doc_count = len(all_files) * 500\n",
    "    final_vocab = load_data(\"vocab.pt\")\n",
    "    \n",
    "    final_df_vector = np.zeros(len(final_vocab))        \n",
    "    temp_vector = np.empty(len(final_vocab))\n",
    "    \n",
    "    \n",
    "    for each_file in all_files:        \n",
    "        df_vector = idf_distance_measure(each_file, 0, 500, final_vocab)\n",
    "        final_df_vector = np.copy(final_df_vector) + np.copy(df_vector)\n",
    "        \n",
    "    print(final_df_vector)    \n",
    "    temp_vector = np.log( total_doc_count / (final_df_vector))\n",
    "    print(temp_vector)    \n",
    "    save_data(temp_vector, \"idf.pt\")\n",
    "    \n",
    "# idf_measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_representation(file_name, no, final_vocab, type=None):\n",
    "    tree = ET.ElementTree(file=file_name)\n",
    "    root = tree.getroot()\n",
    "    rows = root.getchildren()  \n",
    "    \n",
    "    row = rows[no]\n",
    "    textbody = row.attrib['Body']\n",
    "    doc_vocab = remove_tags(textbody.lower()) \n",
    "\n",
    "    if len(doc_vocab) == 0:\n",
    "        return None\n",
    "\n",
    "    doc_vector = np.zeros(len(final_vocab))\n",
    "\n",
    "    # hamming calculation\n",
    "    if type == \"hamming\":            \n",
    "        doc_vocab = list(set(doc_vocab))            \n",
    "        for vocab_iter, each_tok in enumerate(final_vocab):\n",
    "            if each_tok in doc_vocab:\n",
    "                doc_vector[vocab_iter] = 1\n",
    "\n",
    "        #print(doc_vector)\n",
    "        return doc_vector\n",
    "\n",
    "    # euclidean calculation            \n",
    "    elif type == \"euclidean\":\n",
    "        for vocab_iter, each_tok in enumerate(final_vocab):\n",
    "            doc_vector[vocab_iter] = doc_vocab.count(each_tok)\n",
    "\n",
    "        #print(doc_vector)\n",
    "        return doc_vector\n",
    "\n",
    "    # tf-idf calculation\n",
    "    elif type == \"tf\":            \n",
    "        tf_vector = np.zeros(len(final_vocab))            \n",
    "        for vocab_iter, each_tok in enumerate(final_vocab):\n",
    "            tf_vector[vocab_iter] = doc_vocab.count(each_tok)\n",
    "\n",
    "        doc_vector = np.copy(tf_vector) / len(doc_vocab) \n",
    "\n",
    "        idf_vector = load_data(\"idf.pt\")\n",
    "        tf_idf_vector = np.copy(idf_vector) * np.copy(doc_vector) \n",
    "\n",
    "        #print(tf_idf_vector, len(doc_vocab))\n",
    "        return tf_idf_vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_val(type_name):\n",
    "    all_files= glob.glob('./Data/Data/Training/*.xml')\n",
    "    final_vocab = load_data(\"vocab.pt\")\n",
    "    \n",
    "    X_Train, Y_Train, X_val, Y_val, X_Test, Y_Test= [], [], [], [], [], []\n",
    "    \n",
    "    for label_iter, each_file in enumerate(all_files): \n",
    "        for iter in range(0,500):\n",
    "            instance = vector_representation(each_file, iter, final_vocab, type=type_name)\n",
    "            if instance is not None:\n",
    "                X_Train.append(np.copy(instance))\n",
    "                Y_Train.append(label_iter)\n",
    "                \n",
    "        for iter in range(500,700):\n",
    "            instance = vector_representation(each_file, iter, final_vocab, type=type_name)\n",
    "            if instance is not None:\n",
    "                X_val.append(np.copy(instance))\n",
    "                Y_val.append(label_iter)\n",
    "                \n",
    "        for iter in range(700,1200):\n",
    "            instance = vector_representation(each_file, iter, final_vocab, type=type_name)\n",
    "            if instance is not None:\n",
    "                X_Test.append(np.copy(instance))\n",
    "                Y_Test.append(label_iter)\n",
    "                \n",
    "    return X_Train, Y_Train, X_val, Y_val, X_Test, Y_Test\n",
    "\n",
    "def save_dictionary(type):\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = prepare_train_test_val(type)\n",
    "    save_data(X_train, \"X_train_\"+type+\".pt\")\n",
    "    save_data(Y_train, \"Y_train_\"+type+\".pt\")\n",
    "    save_data(X_val, \"X_val_\"+type+\".pt\")\n",
    "    save_data(Y_val, \"Y_val_\"+type+\".pt\")\n",
    "    save_data(X_test, \"X_test_\"+type+\".pt\")\n",
    "    save_data(Y_test, \"Y_test_\"+type+\".pt\")\n",
    "    \n",
    "def load_dictionary(type):\n",
    "    X_train, Y_train, X_test, Y_test = load_data(\"X_train_\"+type+\".pt\"), load_data(\"Y_train_\"+type+\".pt\"), load_data(\"X_test_\"+type+\".pt\"), load_data(\"Y_test_\"+type+\".pt\")\n",
    "    X_val, Y_val = load_data(\"X_val_\"+type+\".pt\"), load_data(\"Y_val_\"+type+\".pt\")\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "# save_dictionary(\"hamming\")\n",
    "# save_dictionary(\"euclidean\")\n",
    "# save_dictionary(\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make prediction of the test points using training points\n",
    "class K_Nearest_Neighbor:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "        self.X_train, self.Y_train, self.X_val, self.Y_val, self.X_test, self.Y_test = load_dictionary(self.type)\n",
    "        \n",
    "    #Hamming Distance calculation between two data points\n",
    "    def hamming_distance(self, instance1, instance2):    \n",
    "        return np.sum(instance1 != instance2)\n",
    "\n",
    "    #Euclidean Distance calculation between two data points\n",
    "    def euclidean_distance(self, instance1, instance2):\n",
    "        return np.linalg.norm(instance1-instance2)\n",
    "\n",
    "    #cos Distance calculation between two data points\n",
    "    def cosine_sim(self, instance1, instance2):\n",
    "        distance = np.dot(instance1, instance2.T) / ( np.linalg.norm(instance1) * np.linalg.norm(instance2))\n",
    "        return distance\n",
    "    \n",
    "    def KNN(self, X_train, Y_train, X_test, n_neighbors=3):\n",
    "\n",
    "        #Determine Number of unique class lebels    \n",
    "        uniqueOutputCount = len(list(set(Y_train)))\n",
    "\n",
    "\n",
    "        allDistances = []\n",
    "        for trainInput, trainActualOutput in zip(X_train, Y_train):\n",
    "            if self.type == \"hamming\":\n",
    "                distance = self.hamming_distance(X_test, trainInput)\n",
    "            elif self.type == \"euclidean\":\n",
    "                distance = self.euclidean_distance(X_test, trainInput)\n",
    "            else:\n",
    "                distance = self.cosine_sim(X_test, trainInput)\n",
    "\n",
    "            allDistances.append((trainInput, trainActualOutput, distance))\n",
    "\n",
    "        #Sort (in ascending order) the training data points based on distances from the test point     \n",
    "        allDistances.sort(key=lambda x: x[2])\n",
    "        if self.type == \"tf\":\n",
    "            allDistances.reverse()\n",
    "\n",
    "\n",
    "        #Assuming output labels are from 0 to uniqueOutputCount-1\n",
    "        voteCount = np.zeros(uniqueOutputCount)\n",
    "        neighbors = []\n",
    "        for n in range(n_neighbors):\n",
    "            neighbors.append(allDistances[n][0])\n",
    "            class_label = int(allDistances[n][1])\n",
    "            voteCount[class_label] += 1\n",
    "\n",
    "        #Determine the Majority Voting (Equal weight considered)\n",
    "        predictedOutput = np.argmax(voteCount)\n",
    "\n",
    "        return predictedOutput, neighbors\n",
    "\n",
    "\n",
    "    def performanceEvaluation(self, X_test, Y_test, n_neighbors=3):\n",
    "        totalCount = 0\n",
    "        correctCount = 0    \n",
    "\n",
    "        for testInput, testActualOutput in zip(X_test, Y_test):\n",
    "            predictedOutput,_ = self.KNN(self.X_train, self.Y_train, testInput, n_neighbors)\n",
    "\n",
    "            if int(predictedOutput) == int(testActualOutput):\n",
    "                #print(\"correct\")\n",
    "                correctCount += 1\n",
    "            #else:\n",
    "                #print(\"wrong\")\n",
    "            totalCount += 1\n",
    "\n",
    "        print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n",
    "        return (correctCount*100)/(totalCount)\n",
    "\n",
    "\n",
    "    def KNN_runner(self): \n",
    "        \n",
    "        val_Acc_all=[]\n",
    "        types = [\"hamming\", \"euclidean\", \"tf\"]\n",
    "        for type in types:\n",
    "            self.change_type(type)\n",
    "            for i in range(1,6,2):\n",
    "                print(\"Running val for type: \", self.type, \" and K = \", i)\n",
    "                val_acc = self.performanceEvaluation(self.X_val, self.Y_val, i)      \n",
    "                val_Acc_all.append([self.type,i,val_acc])\n",
    "        \n",
    "        json_object = json.dumps(val_Acc_all, indent = 4) \n",
    "        with open(\"knn_val_result.json\", \"w\") as outfile: \n",
    "            outfile.write(json_object) \n",
    "                \n",
    "                \n",
    "    def change_type(self, type):\n",
    "        self.type = type\n",
    "        self.X_train, self.Y_train, self.X_val, self.Y_val, self.X_test, self.Y_test = load_dictionary(self.type)\n",
    "        \n",
    "    def test_runner(self, best_n):\n",
    "        \n",
    "        test_Acc_all = [] \n",
    "        \n",
    "        len_all_files= len(glob.glob('./Data/Data/Training/*.xml'))\n",
    "        len_all_files= len_all_files*10\n",
    "        \n",
    "        \n",
    "            \n",
    "        print(\"Running test for type: \", self.type, \" and K = \", best_n)\n",
    "        for i in range(0,len(self.X_test),len_all_files):\n",
    "            test_acc = self.performanceEvaluation(self.X_test[i:(i+len_all_files)], self.Y_test[i:(i+len_all_files)], best_n) \n",
    "            test_Acc_all.append(test_acc)\n",
    "        \n",
    "        json_object = json.dumps(test_Acc_all, indent = 4) \n",
    "        with open(\"knn_best_test.json\", \"w\") as outfile: \n",
    "            outfile.write(json_object) \n",
    "\n",
    "KNN = K_Nearest_Neighbor(type=\"tf\")\n",
    "# KNN.KNN_runner()\n",
    "KNN.test_runner(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    def __init__(self, alpha):\n",
    "        print(\"initialized\")\n",
    "        self.alpha = alpha\n",
    "        self.all_files= glob.glob('./Data/Data/Training/*.xml')\n",
    "        #self.topic_probab = np.zeros(len(self.all_files))        \n",
    "        self.len_final_vocab = len(load_data(\"vocab.pt\"))\n",
    "        \n",
    "        \n",
    "    def change_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        if not os.path.isfile(\"topic_wise_prob_\"+str(alpha)+\".pt\"):\n",
    "            self.word_probab_creation()\n",
    "        \n",
    "    # Parse XML with ElementTree\n",
    "    def parseXML(self, file_name):\n",
    "        tree = ET.ElementTree(file=file_name)\n",
    "        root = tree.getroot()\n",
    "        rows = root.getchildren()\n",
    "        iter = 0\n",
    "        large_vocab = []\n",
    "        for row in rows:\n",
    "            textbody = row.attrib['Body']\n",
    "            file_vocab = remove_tags(textbody.lower())\n",
    "            large_vocab.extend(file_vocab)        \n",
    "            iter += 1\n",
    "            if iter == 500:\n",
    "                #print(len(large_vocab))\n",
    "                return large_vocab          \n",
    "\n",
    "    def word_probab_creation(self):\n",
    "        \n",
    "        final_vocab = load_data(\"vocab.pt\")\n",
    "        len_final_vocab = len(final_vocab)\n",
    "        final_prob = []\n",
    "        \n",
    "        for each_file in self.all_files:\n",
    "            topic_wise_vocab = self.parseXML(each_file)\n",
    "            len_topic_wise_vocab = len(topic_wise_vocab)\n",
    "            topic_wise_prob = {}\n",
    "            \n",
    "            for vocab_iter, each_tok in enumerate(final_vocab):\n",
    "                # total instance of that word divided by total word count, together with smoothing factor\n",
    "                topic_wise_prob[each_tok] = (topic_wise_vocab.count(each_tok) + self.alpha)/ (len_topic_wise_vocab + (self.alpha * len_final_vocab))\n",
    "                \n",
    "            \n",
    "            final_prob.append([topic_wise_prob])\n",
    "            \n",
    "            # Writing to sample.json \n",
    "            # Serializing json  \n",
    "#             json_object = json.dumps(final_prob, indent = 4) \n",
    "#             with open(\"sample_\"+str(self.alpha)+\".json\", \"w\") as outfile: \n",
    "#                 outfile.write(json_object) \n",
    "\n",
    "        save_data(final_prob, \"topic_wise_prob_\"+str(self.alpha)+\".pt\")\n",
    "    \n",
    "    # Parse XML with ElementTree\n",
    "    def single_row(self, file_name, row_num):\n",
    "        tree = ET.ElementTree(file=file_name)\n",
    "        root = tree.getroot()\n",
    "        rows = root.getchildren()\n",
    "        row = rows[row_num]\n",
    "        textbody = row.attrib['Body']\n",
    "        file_vocab = remove_tags(textbody.lower())\n",
    "        return file_vocab\n",
    "    \n",
    "    def get_result(self, file_name, row_num, len_topic_wise_vocab):\n",
    "        words = self.single_row(file_name, row_num)\n",
    "        \n",
    "        if len(words) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        final_prob = load_data(\"topic_wise_prob_\"+str(self.alpha)+\".pt\")\n",
    "        best_acc, best_class = 0, 0       \n",
    "        \n",
    "        allacc = np.zeros(len(self.all_files))\n",
    "        \n",
    "        for label_iter, each_topic_prob in enumerate(final_prob):\n",
    "            current_acc = np.log(1 / len(self.all_files)) #P(c)\n",
    "            #current_acc = 1 / len(self.all_files) #P(c)\n",
    "            \n",
    "            for each_word in words:\n",
    "                try:\n",
    "                    current_acc = current_acc + np.log(np.finfo(float).eps+each_topic_prob[0][each_word])\n",
    "#                     current_acc = current_acc * (each_topic_prob[0][each_word])\n",
    "                except:\n",
    "                    continue\n",
    "                    #current_acc = current_acc + np.log(self.alpha / (len_topic_wise_vocab + (self.alpha * self.len_final_vocab)))\n",
    "            \n",
    "            allacc[label_iter] += current_acc\n",
    "        \n",
    "        best_class = np.argmax(allacc)   \n",
    "        return allacc[best_acc], best_class\n",
    "    \n",
    "    def performanceEvaluation(self, start_iter, end_iter):\n",
    "        totalCount = 0\n",
    "        correctCount = 0        \n",
    "\n",
    "        all_files= glob.glob('./Data/Data/Training/*.xml')\n",
    "        for label_iter, each_file in enumerate(all_files):\n",
    "            topic_wise_vocab = self.parseXML(each_file)\n",
    "            len_topic_wise_vocab = len(topic_wise_vocab)\n",
    "            for iter in range(start_iter,end_iter):\n",
    "                best_acc, best_class = self.get_result(each_file, iter, len_topic_wise_vocab)\n",
    "                if best_class is None:\n",
    "                    continue\n",
    "                totalCount += 1\n",
    "                if best_class == label_iter:\n",
    "                    correctCount += 1 \n",
    "\n",
    "        print(\"Smoothing Factor\",self.alpha,\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n",
    "        return (correctCount*100)/(totalCount)\n",
    "\n",
    "    \n",
    "    def test_runner(self):\n",
    "\n",
    "        print(\"Running test for Smoothing Factor = \", self.alpha)\n",
    "        test_Acc_all = []\n",
    "        for i in range(700,1200,10):\n",
    "            test_acc = self.performanceEvaluation(i, i+10) \n",
    "            test_Acc_all.append(test_acc)\n",
    "        \n",
    "        json_object = json.dumps(test_Acc_all, indent = 4) \n",
    "        with open(\"nb_best_test.json\", \"w\") as outfile: \n",
    "            outfile.write(json_object) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NB_Runner(type):\n",
    "    NB = Naive_Bayes(alpha=0.1)\n",
    "    \n",
    "    if type == \"val\":\n",
    "        smooth_factors = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        val_Acc_all = []\n",
    "        \n",
    "        for smooth_factor in smooth_factors:\n",
    "            NB.change_alpha(smooth_factor)\n",
    "            val_acc = NB.performanceEvaluation(500,700) \n",
    "            val_Acc_all.append([smooth_factor,val_acc])\n",
    "        \n",
    "        json_object = json.dumps(val_Acc_all, indent = 4) \n",
    "        with open(\"nb_val_result.json\", \"w\") as outfile: \n",
    "            outfile.write(json_object) \n",
    "    else:\n",
    "        NB.change_alpha(0.1)\n",
    "        NB.test_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB_Runner(type=\"val\")\n",
    "NB_Runner(type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NB_Runner(type=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\n",
    "def load_json(filename):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f) \n",
    "        return data\n",
    "    \n",
    "KNN_test_results = load_json('knn_best_test.json')\n",
    "NB_test_results = load_json('nb_best_test.json')\n",
    "\n",
    "from scipy import stats\n",
    "print(stats.ttest_rel(NB_test_results,KNN_test_results))\n",
    "print(stats.ttest_rel(KNN_test_results, NB_test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
