{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "        \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RUN_KNN = False\n",
    "RUN_NB = True\n",
    "\n",
    "# all_topics = ['Coffee', 'Arduino', 'Anime']\n",
    "all_topics = ['Anime', 'Arduino', 'Astronomy', 'Biology', 'Chess', 'Coffee', 'Cooking', 'Law', \n",
    "              'Space', 'Windows_Phone', 'Wood_Working']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bangladesh', 'offici', 'peopl', 'republ', 'bangladesh', 'countri', 'south', 'asia', 'eighthmost', 'popul', 'countri', 'world', 'popul', 'exceed', 'million', 'peopl', 'countri', 'better', 'place', 'live']\n"
     ]
    }
   ],
   "source": [
    "text = \"Bangladesh, officially the People's Republic of Bangladesh, is a country in South Asia. It is the eighth-most populous country in the world, with a population exceeding 162 million people. It is not other countries. It is a better place to live in.\"\n",
    "def preprocess_text(text, all_type=True, debug=True):\n",
    "    if debug: print(\"===Raw Text:===\\n\", text)\n",
    "\n",
    "    #1.Lowercase the text\n",
    "    text = text.lower()\n",
    "    if debug: print(\"\\n===After Lowercase:===\\n\", text)\n",
    "\n",
    "    if all_type:\n",
    "        #Number Removal\n",
    "        text = re.sub(r'[-+]?\\d+', '', text)\n",
    "        if debug: print(\"\\n===After Removing Numbers:===\\n\", text)\n",
    "\n",
    "    #2.Remove punctuations\n",
    "    text=text.translate((str.maketrans('','',string.punctuation)))\n",
    "    if debug: print(\"\\n===After Removing Punctuations:===\\n\", text)\n",
    "\n",
    "    #4.Tokenize\n",
    "    text = word_tokenize(text)\n",
    "    if debug: print(\"\\n===After Tokenizing:===\\n\", text)\n",
    "\n",
    "    #3.Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    if debug: print(\"\\n===After Stopword Removal:===\\n\", text)\n",
    "\n",
    "    #6.Lemmatize tokens\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    if debug: print(\"\\n===After Lemmatization:===\\n\", text)\n",
    "\n",
    "    #5.Stemming tokens\n",
    "    stemmer= PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    if debug: print(\"\\n===After Stemming:===\\n\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = preprocess_text(text, all_type=True, debug=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<item name=\"item1\">This is the first line of text.</item>\n",
      "<item name=\"item2\">This is the second line.</item>\n"
     ]
    }
   ],
   "source": [
    "#Following operation is required if you run this cell for the first time\n",
    "#!pip3 install bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "with open('Sample.xml','r',encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    soup = bs(content)\n",
    "    for items in soup.findAll(\"item\"):\n",
    "        print(items)\n",
    "        \n",
    "# https://stackoverflow.com/questions/3351485/how-to-remove-all-html-tags-from-downloaded-page\n",
    "def func_remove_html_tags(document):\n",
    "#     for __tag__ in ['p', 'em', 'i', 'b']:\n",
    "#         document = document.replace('<'+__tag__+'>', '').replace('</'+__tag__+'>', '')\n",
    "#     for __tag__ in ['<p>', '</p>', '<em>', '</em>', '<i>',  '</i>',  '<b>',  '</b>']:\n",
    "#         document = document.replace(__tag__, '')\n",
    "#     print('\\n', i, ':', document)\n",
    "    p = re.compile(r'<.*?>')\n",
    "    document = p.sub('', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_, val_, text_, all_words 500 200 500 28399\n",
      "train_, val_, text_, all_words 500 200 500 38320\n",
      "train_, val_, text_, all_words 500 200 500 33379\n",
      "train_, val_, text_, all_words 500 200 500 33928\n",
      "train_, val_, text_, all_words 500 200 500 26268\n",
      "train_, val_, text_, all_words 500 200 500 36516\n",
      "train_, val_, text_, all_words 500 200 500 17819\n",
      "train_, val_, text_, all_words 500 200 500 40604\n",
      "train_, val_, text_, all_words 500 200 500 34790\n",
      "train_, val_, text_, all_words 500 200 500 19907\n",
      "train_, val_, text_, all_words 500 200 500 31722\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "# topic_name = 'Anime'\n",
    "# remove_html_tags = True\n",
    "# train_cnt, val_cnt, test_cnt = 500, 200, 500\n",
    "\n",
    "def preprocess_from_xml(topic_name, remove_html_tags=True, train_cnt=500, val_cnt=200, test_cnt=500, debug=False):\n",
    "    with open('Data/Training/{}.xml'.format(topic_name),'r',encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = bs(content)\n",
    "        rows = soup.findAll(\"row\")\n",
    "        train_, val_, text_ = [], [], []\n",
    "        all_words = []\n",
    "        tracker_idx = 0\n",
    "        for i, items in enumerate(rows):\n",
    "            document = items['body'].replace('\\n', ' ')\n",
    "            if debug: print('\\n{}: before>'.format(i), document)\n",
    "            if remove_html_tags:\n",
    "                document = func_remove_html_tags(document=document)\n",
    "            if debug: print('after>', document)\n",
    "            document = preprocess_text(text=document, all_type=True, debug=False)\n",
    "            if document == []:\n",
    "                continue\n",
    "            if tracker_idx == train_cnt:\n",
    "                break\n",
    "            train_ += [[document, topic_name]]\n",
    "            all_words += document\n",
    "            tracker_idx += 1\n",
    "        \n",
    "        cnt_temp = tracker_idx\n",
    "        for i, items in enumerate(rows[cnt_temp:]):\n",
    "            document = items['body'].replace('\\n', ' ')\n",
    "            if remove_html_tags:\n",
    "                document = func_remove_html_tags(document=document)\n",
    "            document = preprocess_text(text=document, all_type=True, debug=False)\n",
    "            if document == []:\n",
    "                continue\n",
    "            if tracker_idx == cnt_temp+val_cnt:\n",
    "                break\n",
    "            val_ += [[document, topic_name]]\n",
    "            tracker_idx += 1\n",
    "            \n",
    "        cnt_temp = tracker_idx\n",
    "        for i, items in enumerate(rows[cnt_temp:]):\n",
    "            document = items['body'].replace('\\n', ' ')\n",
    "            if remove_html_tags:\n",
    "                document = func_remove_html_tags(document=document)\n",
    "            document = preprocess_text(text=document, all_type=True, debug=False)\n",
    "            if document == []:\n",
    "                continue\n",
    "            if tracker_idx == cnt_temp+test_cnt:\n",
    "                break\n",
    "            text_ += [[document, topic_name]]\n",
    "            tracker_idx += 1\n",
    "        \n",
    "        print('train_, val_, text_, all_words', train_.__len__(), val_.__len__(), text_.__len__(), all_words.__len__())\n",
    "        return train_, val_, text_, all_words\n",
    "\n",
    "train_docs, val_docs, test_docs = [], [], []\n",
    "all_words_in__all_topics = []\n",
    "topic_wise_word_count = {}\n",
    "for topic_name in all_topics:\n",
    "    train_, val_, text_, all_words_ = preprocess_from_xml(topic_name=topic_name, \n",
    "                                                         remove_html_tags=True, \n",
    "                                                         train_cnt=500, \n",
    "                                                         val_cnt=200, \n",
    "                                                         test_cnt=500, \n",
    "                                                         debug=False)\n",
    "    train_docs += train_\n",
    "    val_docs += val_\n",
    "    test_docs += text_\n",
    "    all_words_in__all_topics += all_words_\n",
    "    topic_wise_word_count[topic_name] = len(all_words_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500 2200 5500 341652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20703,\n",
       " {'Anime': 28399,\n",
       "  'Arduino': 38320,\n",
       "  'Astronomy': 33379,\n",
       "  'Biology': 33928,\n",
       "  'Chess': 26268,\n",
       "  'Coffee': 36516,\n",
       "  'Cooking': 17819,\n",
       "  'Law': 40604,\n",
       "  'Space': 34790,\n",
       "  'Windows_Phone': 19907,\n",
       "  'Wood_Working': 31722})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_docs), len(val_docs), len(test_docs), len(all_words_in__all_topics))\n",
    "def create_dictionary_from_training_data(words):\n",
    "    vocab = {}\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            vocab[w] += 1\n",
    "        else:\n",
    "            vocab[w] = 1\n",
    "    return vocab\n",
    "vocab = create_dictionary_from_training_data(words=all_words_in__all_topics)\n",
    "len(vocab), topic_wise_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(train_docs[1])\n",
    "# print(all_words_in__all_topics[:10])\n",
    "word_position_in_vocab = {}\n",
    "i = 0\n",
    "for w in vocab:\n",
    "    word_position_in_vocab[w] = i\n",
    "    i += 1\n",
    "    \n",
    "VOCAB_SIZE = len(word_position_in_vocab)\n",
    "# VOCAB_SIZE\n",
    "# vocab\n",
    "# word_position_in_vocab\n",
    "\n",
    "total_train_cnt = len(train_docs)\n",
    "def get_IDF(train_docs, word_position_in_vocab, VOCAB_SIZE, alpha=0, beta=1e-5, beta2=1e-5):\n",
    "    D = len(train_docs)\n",
    "#     print('>', word_position_in_vocab)\n",
    "    C_w = np.zeros(VOCAB_SIZE)\n",
    "    for w in word_position_in_vocab:\n",
    "        for document in train_docs:\n",
    "            if w in document[0]:\n",
    "                C_w[word_position_in_vocab[w]] += 1.\n",
    "#                 print(w, document)\n",
    "        C_w[word_position_in_vocab[w]] += 1\n",
    "#         if C_w[word_position_in_vocab[w]] == D:\n",
    "#             C_w[word_position_in_vocab[w]] += beta\n",
    "#     print('C_w', C_w)\n",
    "    IDF = np.log10((D + alpha) / (C_w))\n",
    "    IDF = np.where(IDF<=0, beta2, IDF)\n",
    "#     print(D)\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_doc = 'got got local coffe got'.split(' ')\n",
    "def create_hamming_vect(word_position_in_vocab, VOCAB_SIZE, document):\n",
    "    vect = np.zeros([VOCAB_SIZE])\n",
    "    for w in document:\n",
    "        if w in word_position_in_vocab:\n",
    "            vect[word_position_in_vocab[w]] = 1.\n",
    "    return vect\n",
    "\n",
    "# vect = create_hamming_vect(word_position_in_vocab=word_position_in_vocab, VOCAB_SIZE=VOCAB_SIZE, document=dummy_doc)  \n",
    "# print(sum(vect), len(dummy_doc), len(vect))\n",
    "\n",
    "def create_euclidean_vect(word_position_in_vocab, VOCAB_SIZE, document):\n",
    "    vect = np.zeros([VOCAB_SIZE])\n",
    "    for w in document:\n",
    "        if w in word_position_in_vocab:\n",
    "            vect[word_position_in_vocab[w]] += 1.\n",
    "    return vect\n",
    "\n",
    "# vect = create_euclidean_vect(word_position_in_vocab=word_position_in_vocab, VOCAB_SIZE=VOCAB_SIZE, document=dummy_doc)  \n",
    "# print(sum(vect), len(dummy_doc), len(vect))\n",
    "\n",
    "def create_tfIdf_vect(word_position_in_vocab, VOCAB_SIZE, document, IDF):\n",
    "    TF = np.zeros([VOCAB_SIZE])\n",
    "    total_ = 0.\n",
    "    if document == []:\n",
    "        print('No words in doc. Skipping...')\n",
    "        return TF\n",
    "    for w in document:\n",
    "        if w in word_position_in_vocab:\n",
    "#             print(w)\n",
    "            total_ += 1. # jokhon unknown word totally consider korbo na. total word count er moddheo na\n",
    "            TF[word_position_in_vocab[w]] += 1.\n",
    "#     print('TF.sum():', TF.sum())\n",
    "    TF /= total_ # TF(d, w) = N(d, w) / W(d)\n",
    "#     print(IDF.shape)\n",
    "#     print('TF:', TF)\n",
    "    TF_IDF = TF * IDF\n",
    "#     print(TF_IDF.shape)\n",
    "    return TF_IDF\n",
    "\n",
    "# def create_tfIdf_vect2(word_position_in_vocab, VOCAB_SIZE, document, IDF):\n",
    "#     TF = {}\n",
    "#     total_ = 0.\n",
    "#     for w in document:\n",
    "#         if w not in word_position_in_vocab: # During TF calculation, You can simply omit the word when the word is new in test/validation set \n",
    "#             continue\n",
    "#         total_ += 1.\n",
    "#         if w in TF:\n",
    "#             TF[w] += 1.\n",
    "#         else:\n",
    "#             TF[w] = 1.\n",
    "#     for w in TF:\n",
    "#         TF[w] /= total_ # TF(d, w) = N(d, w) / W(d)\n",
    "#         TF[w] *= IDF[w]\n",
    "#     return TF # tf_idf\n",
    "\n",
    "# vect = create_tfIdf_vect(word_position_in_vocab=word_position_in_vocab, VOCAB_SIZE=VOCAB_SIZE, document=dummy_doc, IDF=IDF)  \n",
    "# print(sum(vect), len(dummy_doc), len(vect))\n",
    "\n",
    "\n",
    "# def create_hamming_vect(vocab, document):\n",
    "#     vect = []\n",
    "#     for w in vocab:\n",
    "#         vect += [int(w in document)]\n",
    "#     return vect\n",
    "\n",
    "# vect = create_hamming_vect(vocab=vocab, document=train_docs[0][0])  \n",
    "# sum(vect), len(train_docs[0][0]), len(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.log10(4/3)\n",
    "# # np.save('IDF.npy', IDF)\n",
    "# # debug..\n",
    "# dummy_docs = [['play cricket play football'.split(), 'sports'],\n",
    "#               ['play music'.split(), 'music'],\n",
    "#               ['like singing'.split(), 'music'],\n",
    "#               ['cricket very small insect'.split(), 'biology'],\n",
    "#               ['want play cricket'.split(), None]\n",
    "#              ]\n",
    "# dummy_vocab_pos = {'play':0,'cricket':1,'football':2,'music':3,'singing':4,'very':5,'small':6,'insect':7,'like':8}\n",
    "# IDF = get_IDF(train_docs=dummy_docs[:-1], \n",
    "#               word_position_in_vocab=dummy_vocab_pos,\n",
    "#               VOCAB_SIZE=len(dummy_vocab_pos), \n",
    "#               alpha=0, \n",
    "#               beta=1e-5)\n",
    "# # print(IDF.max(), IDF.min(), IDF.mean())\n",
    "# print(IDF)\n",
    "\n",
    "# vect = create_tfIdf_vect(word_position_in_vocab=dummy_vocab_pos, \n",
    "#                          VOCAB_SIZE=len(dummy_vocab_pos), \n",
    "#                          document=dummy_docs[-1][0], \n",
    "#                          IDF=IDF)  \n",
    "# print(vect*2/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RUN_KNN:\n",
    "    IDF = get_IDF(train_docs=train_docs, \n",
    "                  word_position_in_vocab=word_position_in_vocab,\n",
    "                  VOCAB_SIZE=VOCAB_SIZE, \n",
    "                  alpha=0, \n",
    "                  beta=1e-5)\n",
    "    IDF.min(), IDF.max(), IDF.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance2(instance1, instance2): # provided code\n",
    "    distance = 0.0\n",
    "    for i in range(len(instance1)):\n",
    "        distance += (instance1[i] - instance2[i])**2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "def euclidean_distance(instance1, instance2):\n",
    "    distance = instance1 - instance2\n",
    "    return np.sqrt(np.dot(distance, distance))\n",
    "\n",
    "# a, b = np.array([1,2,3,6]),  np.array([5,6,7,8])\n",
    "# print(euclidean_distance2(a,b), euclidean_distance(a,b))\n",
    "\n",
    "def hamming_distance(bin_vect1, bin_vect2): ##??\n",
    "    bin_vect1, bin_vect2 = bin_vect1.astype(int), bin_vect2.astype(int)\n",
    "    return np.sum(np.array(bin_vect1)^np.array(bin_vect2))\n",
    "\n",
    "# a, b = np.array([0,0,0,1,1]), np.array([1,1,1,1,1])\n",
    "# print(hamming_distance(a,b))\n",
    "\n",
    "def cosine_similarity(instance1, instance2):\n",
    "    return np.dot(instance1, instance2) / (np.linalg.norm(instance1) * np.linalg.norm(instance2))\n",
    "\n",
    "\n",
    "a, b = np.array([1,1,0,0]),  np.array([1,0,0,0])\n",
    "print(cosine_similarity(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make prediction of the test points using training points\n",
    "\n",
    "class KNN_Text_Classifier:\n",
    "    def __init__(self, train_docs):\n",
    "        self.train_hamming_vect = []\n",
    "        self.train_euclid_vect = []\n",
    "        self.train_cosine_vect = []\n",
    "        self.train_labels = []\n",
    "        self.train_docs = train_docs\n",
    "        \n",
    "        for i, doc in enumerate(train_docs):\n",
    "            self.train_labels += [doc[1]]\n",
    "            if doc[0] == []:\n",
    "                print(i)\n",
    "                continue\n",
    "            self.train_hamming_vect += [create_hamming_vect(word_position_in_vocab=word_position_in_vocab, \n",
    "                                                       VOCAB_SIZE=VOCAB_SIZE, document=doc[0])]\n",
    "            self.train_euclid_vect += [create_euclidean_vect(word_position_in_vocab=word_position_in_vocab, \n",
    "                                                        VOCAB_SIZE=VOCAB_SIZE, document=doc[0])]\n",
    "            self.train_cosine_vect += [create_tfIdf_vect(word_position_in_vocab=word_position_in_vocab, \n",
    "                                                    VOCAB_SIZE=VOCAB_SIZE, document=doc[0], IDF=IDF)]\n",
    "        self.unique_labels = list(set(self.train_labels))\n",
    "        self.unique_labels_cnt = len(self.unique_labels)\n",
    "        self.label_positions_in_unique_labels = {}\n",
    "        for i, label in enumerate(self.unique_labels):\n",
    "            self.label_positions_in_unique_labels[label] = i\n",
    "            \n",
    "#         self.voteCount = np.zeros([self.unique_labels_cnt])\n",
    "\n",
    "    def knn_prediction_single_sample(self, doc_test, dist_type, n_neighbors): #based on given code\n",
    "        if dist_type == 0:\n",
    "            dist_func = hamming_distance\n",
    "            X_train = self.train_hamming_vect\n",
    "            testInput = create_hamming_vect(word_position_in_vocab=word_position_in_vocab, \n",
    "                                          VOCAB_SIZE=VOCAB_SIZE, document=doc_test)\n",
    "        if dist_type == 1:\n",
    "            dist_func = euclidean_distance\n",
    "            X_train = self.train_euclid_vect\n",
    "            testInput = create_euclidean_vect(word_position_in_vocab=word_position_in_vocab, \n",
    "                                          VOCAB_SIZE=VOCAB_SIZE, document=doc_test)\n",
    "        if dist_type == 2:\n",
    "            dist_func = cosine_similarity\n",
    "            X_train = self.train_cosine_vect\n",
    "            testInput = create_tfIdf_vect(word_position_in_vocab=word_position_in_vocab, \n",
    "                                        VOCAB_SIZE=VOCAB_SIZE, document=doc_test, IDF=IDF)\n",
    "\n",
    "        #calculate for earch test data points\n",
    "        allDistances = []\n",
    "        for i, (trainInput, trainActualOutput) in enumerate(zip(X_train, self.train_labels)):\n",
    "            distance = dist_func(testInput, trainInput)\n",
    "            allDistances.append((i, trainActualOutput, distance))\n",
    "        #Sort (in ascending for 0 and 1, descending for 2) the training data points based on distances from the test point\n",
    "        allDistances.sort(key=lambda x: x[2], reverse = dist_type==2)\n",
    "#         print(len(allDistances))\n",
    "#         if dist_type==2:\n",
    "#             print(allDistances[:10], allDistances[-10:])\n",
    "\n",
    "        #Assuming output labels are from 0 to self.unique_labels_cnt-1\n",
    "        voteCount = np.zeros(self.unique_labels_cnt)\n",
    "        neighbor_indices = []\n",
    "        for n in range(n_neighbors):\n",
    "            neighbor_indices.append(allDistances[n][0])\n",
    "            voteCount[self.label_positions_in_unique_labels[allDistances[n][1]]] += 1\n",
    "\n",
    "        #Determine the Majority Voting (Equal weight considered)\n",
    "#         print('voteCount:', voteCount)\n",
    "        predictedOutput = np.argmax(voteCount)\n",
    "        predictedOutput = self.unique_labels[predictedOutput]\n",
    "\n",
    "        return predictedOutput, neighbor_indices\n",
    "\n",
    "\n",
    "    def performanceEvaluation(self, test_docs, dist_type, n_neighbors):\n",
    "        totalCount = 0\n",
    "        correctCount = 0\n",
    "\n",
    "        for i, doc_test_and_label in enumerate(test_docs):\n",
    "            if doc_test_and_label[0] == []:\n",
    "                print(i)\n",
    "                continue\n",
    "            predictedOutput, _ = self.knn_prediction_single_sample(doc_test_and_label[0], dist_type, n_neighbors)\n",
    "\n",
    "            if predictedOutput == doc_test_and_label[1]:\n",
    "                correctCount += 1\n",
    "            totalCount += 1\n",
    "#             print(predictedOutput, doc_test_and_label[1])\n",
    "\n",
    "        print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RUN_KNN:\n",
    "    knn = KNN_Text_Classifier(train_docs=train_docs)\n",
    "    knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RUN_KNN:\n",
    "    print('VAL::')\n",
    "    print('hamming')\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=0, n_neighbors=1)\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=0, n_neighbors=3)\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=0, n_neighbors=5)\n",
    "    print('\\neuclidean')\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=1, n_neighbors=1)\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=1, n_neighbors=3)\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=1, n_neighbors=5)\n",
    "    print('\\ncosine')\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=2, n_neighbors=1)\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=2, n_neighbors=3)\n",
    "    knn.performanceEvaluation(val_docs[:], dist_type=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RUN_KNN:\n",
    "    print('TEST::')\n",
    "    print('hamming')\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=0, n_neighbors=1)\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=0, n_neighbors=3)\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=0, n_neighbors=5)\n",
    "    print('\\neuclidean')\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=1, n_neighbors=1)\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=1, n_neighbors=3)\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=1, n_neighbors=5)\n",
    "    print('\\ncosine')\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=2, n_neighbors=1)\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=2, n_neighbors=3)\n",
    "    knn.performanceEvaluation(test_docs[:], dist_type=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class NB_Text_Classifier:\n",
    "    def __init__(self, train_docs, topic_wise_word_count, smoothing_factor, consider_oov):\n",
    "        self.total_samples = len(train_docs)\n",
    "        self.consider_oov = consider_oov\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.probab_topic = {}\n",
    "        self.probab_word_given_topic = {}\n",
    "        self.topic_wise_word_count = topic_wise_word_count\n",
    "#         self.min_probab = smoothing_factor / smoothing_factor*VOCAB_SIZE # for cases when ð‘_(ð‘¤_ð‘–,ð¶_ð‘˜ )=0  â‡’ð‘(ð‘¤_ð‘–â”‚ð¶_ð‘˜ )=0 â‡’ð‘(ð¶_ð‘˜ |ð·_ð‘¡ )=0 \n",
    "        dummy_topic_map_for_words = {}\n",
    "        for topic in topic_wise_word_count:\n",
    "            dummy_topic_map_for_words[topic] = smoothing_factor\n",
    "        \n",
    "        for i, doc in enumerate(train_docs):\n",
    "            if doc[0] == []:\n",
    "                print(i)\n",
    "                continue\n",
    "            if doc[1] in self.probab_topic:\n",
    "                self.probab_topic[doc[1]] += 1.\n",
    "            else:\n",
    "                self.probab_topic[doc[1]] = 1.\n",
    "            \n",
    "            for w in doc[0]:\n",
    "                if w not in self.probab_word_given_topic:\n",
    "                    self.probab_word_given_topic[w] = deepcopy(dummy_topic_map_for_words)\n",
    "                self.probab_word_given_topic[w][doc[1]] += 1.\n",
    "        \n",
    "        for w in self.probab_word_given_topic:\n",
    "            for topic in topic_wise_word_count:\n",
    "                self.probab_word_given_topic[w][topic] /= (topic_wise_word_count[topic] + smoothing_factor * VOCAB_SIZE)\n",
    "        \n",
    "        for topic in topic_wise_word_count:\n",
    "            self.probab_topic[topic] /= self.total_samples\n",
    "            \n",
    "    def nb_prediction_single_sample(self, doc_test): #based on given code\n",
    "        prediction = [0.0, None] # probab, predicted_topic\n",
    "        for topic in self.probab_topic:\n",
    "            temp_probab = self.predict_probability(topic=topic, doc=doc_test)\n",
    "            if temp_probab >= prediction[0]:\n",
    "                prediction = [temp_probab, topic]\n",
    "        return prediction\n",
    "            \n",
    "    def predict_probability(self, topic, doc): # P(C|x1,x2,...) = P(C) * P(x1|C) * P(x2|C) * ... / CONSTANT\n",
    "        probab = self.probab_topic[topic] # P(C)\n",
    "        for w in doc:\n",
    "            if w not in vocab:\n",
    "                if self.consider_oov:\n",
    "                    probab *= self.smoothing_factor / (self.topic_wise_word_count[topic] + self.smoothing_factor * VOCAB_SIZE)\n",
    "                continue\n",
    "            probab *= self.probab_word_given_topic[w][topic]\n",
    "        return probab\n",
    "    \n",
    "    def performanceEvaluation(self, test_docs):\n",
    "        totalCount = 0\n",
    "        correctCount = 0\n",
    "\n",
    "        for i, doc_test_and_label in enumerate(test_docs):\n",
    "            if doc_test_and_label[0] == []:\n",
    "                print(i)\n",
    "                continue\n",
    "            predictedOutput = self.nb_prediction_single_sample(doc_test_and_label[0])\n",
    "\n",
    "            if predictedOutput[1] == doc_test_and_label[1]:\n",
    "                correctCount += 1\n",
    "#             else:\n",
    "#                 print(predictedOutput[1], doc_test_and_label[1])\n",
    "            totalCount += 1\n",
    "#             print(predictedOutput, doc_test_and_label[1])\n",
    "\n",
    "        print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))    \n",
    "        return (correctCount*100)/(totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_NB:\n",
    "    nb = NB_Text_Classifier(train_docs=train_docs, \n",
    "                            topic_wise_word_count=topic_wise_word_count, \n",
    "                            smoothing_factor=1., \n",
    "                            consider_oov=False) #Î±: smoothing factor. If Î± = 1, then we call it Laplace Smoothing Factor. If Î± <1, then we call it Lidstone Smoothing Factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing_factor: 0.1\n",
      "Total Correct Count:  1656  Total Wrong Count:  544  Accuracy:  75.27272727272727\n",
      "Total Correct Count:  4190  Total Wrong Count:  1310  Accuracy:  76.18181818181819\n",
      "smoothing_factor: 0.2\n",
      "Total Correct Count:  1651  Total Wrong Count:  549  Accuracy:  75.04545454545455\n",
      "Total Correct Count:  4178  Total Wrong Count:  1322  Accuracy:  75.96363636363637\n",
      "smoothing_factor: 0.3\n",
      "Total Correct Count:  1650  Total Wrong Count:  550  Accuracy:  75.0\n",
      "Total Correct Count:  4171  Total Wrong Count:  1329  Accuracy:  75.83636363636364\n",
      "smoothing_factor: 0.4\n",
      "Total Correct Count:  1644  Total Wrong Count:  556  Accuracy:  74.72727272727273\n",
      "Total Correct Count:  4168  Total Wrong Count:  1332  Accuracy:  75.78181818181818\n",
      "smoothing_factor: 0.5\n",
      "Total Correct Count:  1641  Total Wrong Count:  559  Accuracy:  74.5909090909091\n",
      "Total Correct Count:  4156  Total Wrong Count:  1344  Accuracy:  75.56363636363636\n",
      "smoothing_factor: 0.6\n",
      "Total Correct Count:  1632  Total Wrong Count:  568  Accuracy:  74.18181818181819\n",
      "Total Correct Count:  4150  Total Wrong Count:  1350  Accuracy:  75.45454545454545\n",
      "smoothing_factor: 0.7\n",
      "Total Correct Count:  1629  Total Wrong Count:  571  Accuracy:  74.04545454545455\n",
      "Total Correct Count:  4141  Total Wrong Count:  1359  Accuracy:  75.2909090909091\n",
      "smoothing_factor: 0.8\n",
      "Total Correct Count:  1625  Total Wrong Count:  575  Accuracy:  73.86363636363636\n",
      "Total Correct Count:  4126  Total Wrong Count:  1374  Accuracy:  75.01818181818182\n",
      "smoothing_factor: 0.9\n",
      "Total Correct Count:  1620  Total Wrong Count:  580  Accuracy:  73.63636363636364\n",
      "Total Correct Count:  4114  Total Wrong Count:  1386  Accuracy:  74.8\n",
      "smoothing_factor: 1.0\n",
      "Total Correct Count:  1619  Total Wrong Count:  581  Accuracy:  73.5909090909091\n",
      "Total Correct Count:  4100  Total Wrong Count:  1400  Accuracy:  74.54545454545455\n",
      "[0.1, 75.27272727272727]\n"
     ]
    }
   ],
   "source": [
    "if RUN_NB:\n",
    "    # nb.probab_topic, nb.probab_word_given_topic\n",
    "    # nb.predict_probability(list(topic_wise_word_count)[0], test_docs[0][0])\n",
    "    GRANULARITY = 10\n",
    "    best_smoothing_factor = [-1, -1]\n",
    "    for x in range(1, GRANULARITY+1):\n",
    "        smoothing_factor = x/GRANULARITY\n",
    "        print('smoothing_factor:', smoothing_factor)\n",
    "        nb = NB_Text_Classifier(train_docs=train_docs, \n",
    "                                topic_wise_word_count=topic_wise_word_count, \n",
    "                                smoothing_factor=smoothing_factor,\n",
    "                                consider_oov=False) # where to consider out of vocab words during computation\n",
    "        val_acc = nb.performanceEvaluation(test_docs=val_docs)\n",
    "        test_acc = nb.performanceEvaluation(test_docs=test_docs)\n",
    "        if val_acc > best_smoothing_factor[1]:\n",
    "            best_smoothing_factor = [smoothing_factor, val_acc]\n",
    "    print(best_smoothing_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 / np.array([2,2,2])\n",
    "# word_position_in_vocab\n",
    "# [1, 2, 3, 4, 1, 4, 1].count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt=0\n",
    "# for document in train_docs:\n",
    "#     if 'got' in document[0]:\n",
    "#         cnt+=1\n",
    "# cnt, C_w[0], D, D/C_w[0], IDF[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.sum(np.array([1.,1.,0.,1.]).astype(int)^np.array([0.,0.,0.,1.]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = 1\n",
    "# b = a==1\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
